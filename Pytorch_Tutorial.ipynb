{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_Tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMKedxM9fQ/ApNv2aavfarO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bigtimecodersean/Biodiverstiy/blob/main/Pytorch_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Super Basics "
      ],
      "metadata": {
        "id": "tPD3wBM5H3L5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j7WTYgAPEGfm"
      },
      "outputs": [],
      "source": [
        "import torch "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(2,2)\n",
        "y = torch.rand(2,2)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mzjkd4UCEIF0",
        "outputId": "fd75a4cf-d476-4c3c-b3fe-d7951d0b6a96"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3146, 0.1283],\n",
            "        [0.4089, 0.9612]])\n",
            "tensor([[0.4810, 0.7168],\n",
            "        [0.4452, 0.6967]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = x*y\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4ysDHStEOpe",
        "outputId": "aaba58bc-c310-4d26-c8aa-bcbfc6e82fb4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1513, 0.0920],\n",
            "        [0.1821, 0.6696]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Slicing\n",
        "print(z[1,1].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGhA18J2FPv2",
        "outputId": "ac7994e0-3ce7-4907-e24c-57e34d6ee91a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6696261763572693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshaping\n",
        "x = torch.rand(4,4)\n",
        "y = x.view(2,8)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oMyZnygFsb2",
        "outputId": "ed3d6442-4255-4116-fef6-61351eb40824"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.7164, 0.0918, 0.0026, 0.2648],\n",
            "        [0.4866, 0.4327, 0.1794, 0.5089],\n",
            "        [0.1519, 0.9823, 0.4226, 0.5413],\n",
            "        [0.5599, 0.3142, 0.5560, 0.3547]])\n",
            "tensor([[0.7164, 0.0918, 0.0026, 0.2648, 0.4866, 0.4327, 0.1794, 0.5089],\n",
            "        [0.1519, 0.9823, 0.4226, 0.5413, 0.5599, 0.3142, 0.5560, 0.3547]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#From numpy to torch \n",
        "import numpy as np\n",
        "a = np.ones(5)\n",
        "print(a)\n",
        "b = torch.from_numpy(a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvThS3XLF1mk",
        "outputId": "bb61b1a5-db61-4983-a0ea-6eb67f58d771"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. 1. 1.]\n",
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a += 1"
      ],
      "metadata": {
        "id": "HN_ftlwjGxhR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgj-bHhaG3RR",
        "outputId": "34e685ea-a39a-4366-c299-e4a3be45bfd4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5dqKaT9G36U",
        "outputId": "71b8eefd-336f-47b6-cc86-eb555fec8126"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "0n455eE1G4g_",
        "outputId": "5282c5eb-9833-412a-f5c8-1514908b4113"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-4f66399b0155>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    if torch.cuda.is_available():\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Numpy can only handle CPU tensors, not GPU \n",
        "\n",
        "#include requires_grade = True to tell Pytorch that you will require gardients to be tracked later "
      ],
      "metadata": {
        "id": "6x2ZCVUVG_b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Gradients"
      ],
      "metadata": {
        "id": "ETqyAC-0H1Ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad = True)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu5WAPH-H8WC",
        "outputId": "f5ebeaf0-8278-4c39-d3eb-aa9a7709f475"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.7246, 1.4243, 0.3129], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x+2"
      ],
      "metadata": {
        "id": "4j-dxCrgH_tp"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TmJjRFcKcEr",
        "outputId": "9201dbbe-b1d4-42aa-cc4a-39a8a8f78434"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.7246, 3.4243, 2.3129], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = y*y*2"
      ],
      "metadata": {
        "id": "nf4_JbygKd34"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IJ8amNTrKpwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIZUepAuKsm6",
        "outputId": "984fcb70-c3d6-40af-bbe4-525bf7968f23"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([14.8471, 23.4520, 10.6994], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = torch.tensor([.1, 1.0, .001], dtype = torch.float32)"
      ],
      "metadata": {
        "id": "089wtNpXKtds"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBIFGUfFKyv0",
        "outputId": "514b2099-1393-4642-d675-b53a932a3538"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(16.3328, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward(v) #dz/dx "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "ePTCe9gHKzuq",
        "outputId": "54b29236-3c39-4ef2-ede9-faedb842d6d5"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-9e37371cdefa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#dz/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prevent python from tracking gradient history with grad_fn attribute "
      ],
      "metadata": {
        "id": "B7zyRtXXLDjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6teONHPKNOD-",
        "outputId": "a671ad77-2834-442d-9c6f-7a74d7d51a2b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.7246, 1.4243, 0.3129], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x.detach()\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJS2f-5FNP8p",
        "outputId": "0cbd491b-fda7-4ca8-af06-43cfba701f73"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.7246, 1.4243, 0.3129])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.requires_grad_(True)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-d2P10_NSrT",
        "outputId": "cbb1d080-7e56-45f6-8e38-f92983c69936"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.7246, 1.4243, 0.3129], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  z = x+2\n",
        "  print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mApEPevDNeOZ",
        "outputId": "e45cacdd-d914-42e9-c9ed-233543111f9c"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.7246, 3.4243, 2.3129])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "nzyKZvj8Ob1w"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Empty gradients at each step  Incorrect gradidents"
      ],
      "metadata": {
        "id": "BMTz-DKncv9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "weights = torch.ones(4, requires_grad = True)\n",
        "for epoch in range(3):\n",
        "  model_output = (weights*3).sum()\n",
        "\n",
        "  model_output.backward()\n",
        "\n",
        "  print(weights.grad)\n",
        "\n",
        "  weights.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGKya4KnNkbQ",
        "outputId": "0a05d5bc-b446-4d1c-f100-2052d92288cb"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing the same as above but in Optimizer: "
      ],
      "metadata": {
        "id": "tLsEyFgfdeYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.ones(4, requires_grad = True)\n",
        "optimizer = torch.optim.SGD(weights,lr=.01)\n",
        "optimizer.step()\n",
        "optimizer.zero_grad()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "6871fDgvOZm0",
        "outputId": "41bafa18-3fb2-42a8-8cf3-0f8e06e3edc1"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-09b5c9c01301>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov, maximize)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdampening\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nesterov momentum requires a momentum and zero dampening\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     40\u001b[0m             raise TypeError(\"params argument given to the optimizer should be \"\n\u001b[1;32m     41\u001b[0m                             \u001b[0;34m\"an iterable of Tensors or dicts, but got \"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                             torch.typename(params))\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: params argument given to the optimizer should be an iterable of Tensors or dicts, but got torch.FloatTensor"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7wu6Drbvdn4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Backpropagation & Calculating gradients "
      ],
      "metadata": {
        "id": "6kWTWJh8eM2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.tensor(1.0)\n",
        "y= torch.tensor(2.0)\n",
        "w = torch.tensor(1.0, requires_grad = True)\n",
        "#forward pass and compute loss\n",
        "\n",
        "y_hat = w*x\n",
        "loss = (y_hat-y)**2\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raqMsQ2jeP9x",
        "outputId": "c86d0c4f-f934-4e39-ab8d-24e9071f0227"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Backward Pass\n",
        "loss.backward()\n",
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_iq0gGxjHZX",
        "outputId": "a6a7d1b0-9ba6-44c5-c987-de41bd956932"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the first gradient after the forward and backward pass "
      ],
      "metadata": {
        "id": "LJbKLR-bjO5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###Update weights\n",
        "###next forward and backward pass "
      ],
      "metadata": {
        "id": "mtQTz9OZjMVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent Using Numpy\n"
      ],
      "metadata": {
        "id": "Xti0kkLyjeaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "qj_eFDCOj8n8"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#f = w*x\n",
        "#f = 2*x\n",
        "X = np.array([1,2,3,4], dtype = np.float32)\n",
        "Y = np.array([2,4,6,8], dtype = np.float32)\n",
        "\n",
        "#initialize w: \n",
        "w = 0.0\n",
        "\n",
        "#model prediction\n",
        "\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "#calculate loss = MSE\n",
        "def loss(y,y_predicted):\n",
        "  return ((y_predicted-y)**2).mean()\n",
        "\n",
        "#Calculate gradient \n",
        "#MSE = 1/N*(w*x - y)**2\n",
        "#dJ/dw = 1/N*2x*(w*x-y)\n",
        "def gradient(x,y,y_predicted):\n",
        "  return np.dot(2*x,y_predicted-y).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "#Training \n",
        "learning_rate = .01\n",
        "n_iters = 20\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #prediction = ford pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  #loss \n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  #Gradients\n",
        "  dw = gradient(X,Y, y_pred)\n",
        "\n",
        "  #UPdate weights \n",
        "  w -= learning_rate*dw\n",
        "\n",
        "  if epoch % 1 == 0: \n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYUT5GDXj9WH",
        "outputId": "4c2539f0-f627-4163-ccfd-8623707fda58"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 1.200, loss = 30.00000000\n",
            "epoch 2: w = 1.680, loss = 4.79999924\n",
            "epoch 3: w = 1.872, loss = 0.76800019\n",
            "epoch 4: w = 1.949, loss = 0.12288000\n",
            "epoch 5: w = 1.980, loss = 0.01966083\n",
            "epoch 6: w = 1.992, loss = 0.00314574\n",
            "epoch 7: w = 1.997, loss = 0.00050331\n",
            "epoch 8: w = 1.999, loss = 0.00008053\n",
            "epoch 9: w = 1.999, loss = 0.00001288\n",
            "epoch 10: w = 2.000, loss = 0.00000206\n",
            "epoch 11: w = 2.000, loss = 0.00000033\n",
            "epoch 12: w = 2.000, loss = 0.00000005\n",
            "epoch 13: w = 2.000, loss = 0.00000001\n",
            "epoch 14: w = 2.000, loss = 0.00000000\n",
            "epoch 15: w = 2.000, loss = 0.00000000\n",
            "epoch 16: w = 2.000, loss = 0.00000000\n",
            "epoch 17: w = 2.000, loss = 0.00000000\n",
            "epoch 18: w = 2.000, loss = 0.00000000\n",
            "epoch 19: w = 2.000, loss = 0.00000000\n",
            "epoch 20: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qNOmADi5nUA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, with Pytorch "
      ],
      "metadata": {
        "id": "1EzOYyOinUfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "\n",
        "\n",
        "#f = w*x\n",
        "#f = 2*x\n",
        "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
        "Y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
        "\n",
        "#initialize w: \n",
        "w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True)\n",
        "\n",
        "#model prediction is the same\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "#calculate loss = MSE is the same\n",
        "def loss(y,y_predicted):\n",
        "  return ((y_predicted-y)**2).mean()\n",
        "\n",
        "#Calculate gradient \n",
        "\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "#Training \n",
        "learning_rate = .01\n",
        "n_iters = 100\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #prediction = ford pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  #loss \n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  #Gradients = backward pass\n",
        "  l.backward() #dl/dw\n",
        "\n",
        "  #UPdate weights \n",
        "  #Don't want this opreationo be part of the computational graph ... wrap\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate*w.grad\n",
        "\n",
        "  #Must empty gradients, because whenever we write backward, we accumulate them in the w.grad_ attributes... so need to make sure gradients are 0 before next iteration \n",
        "  w.grad.zero_()\n",
        "  if epoch % 1 == 0: \n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfPYdSjGnWtP",
        "outputId": "37be83fe-e542-4478-94e9-902992f96adc"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 2: w = 0.555, loss = 21.67499924\n",
            "epoch 3: w = 0.772, loss = 15.66018772\n",
            "epoch 4: w = 0.956, loss = 11.31448650\n",
            "epoch 5: w = 1.113, loss = 8.17471695\n",
            "epoch 6: w = 1.246, loss = 5.90623236\n",
            "epoch 7: w = 1.359, loss = 4.26725292\n",
            "epoch 8: w = 1.455, loss = 3.08308983\n",
            "epoch 9: w = 1.537, loss = 2.22753215\n",
            "epoch 10: w = 1.606, loss = 1.60939169\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 12: w = 1.716, loss = 0.84011245\n",
            "epoch 13: w = 1.758, loss = 0.60698116\n",
            "epoch 14: w = 1.794, loss = 0.43854395\n",
            "epoch 15: w = 1.825, loss = 0.31684780\n",
            "epoch 16: w = 1.851, loss = 0.22892261\n",
            "epoch 17: w = 1.874, loss = 0.16539653\n",
            "epoch 18: w = 1.893, loss = 0.11949898\n",
            "epoch 19: w = 1.909, loss = 0.08633806\n",
            "epoch 20: w = 1.922, loss = 0.06237914\n",
            "epoch 21: w = 1.934, loss = 0.04506890\n",
            "epoch 22: w = 1.944, loss = 0.03256231\n",
            "epoch 23: w = 1.952, loss = 0.02352631\n",
            "epoch 24: w = 1.960, loss = 0.01699772\n",
            "epoch 25: w = 1.966, loss = 0.01228084\n",
            "epoch 26: w = 1.971, loss = 0.00887291\n",
            "epoch 27: w = 1.975, loss = 0.00641066\n",
            "epoch 28: w = 1.979, loss = 0.00463169\n",
            "epoch 29: w = 1.982, loss = 0.00334642\n",
            "epoch 30: w = 1.985, loss = 0.00241778\n",
            "epoch 31: w = 1.987, loss = 0.00174685\n",
            "epoch 32: w = 1.989, loss = 0.00126211\n",
            "epoch 33: w = 1.991, loss = 0.00091188\n",
            "epoch 34: w = 1.992, loss = 0.00065882\n",
            "epoch 35: w = 1.993, loss = 0.00047601\n",
            "epoch 36: w = 1.994, loss = 0.00034392\n",
            "epoch 37: w = 1.995, loss = 0.00024848\n",
            "epoch 38: w = 1.996, loss = 0.00017952\n",
            "epoch 39: w = 1.996, loss = 0.00012971\n",
            "epoch 40: w = 1.997, loss = 0.00009371\n",
            "epoch 41: w = 1.997, loss = 0.00006770\n",
            "epoch 42: w = 1.998, loss = 0.00004891\n",
            "epoch 43: w = 1.998, loss = 0.00003534\n",
            "epoch 44: w = 1.998, loss = 0.00002553\n",
            "epoch 45: w = 1.999, loss = 0.00001845\n",
            "epoch 46: w = 1.999, loss = 0.00001333\n",
            "epoch 47: w = 1.999, loss = 0.00000963\n",
            "epoch 48: w = 1.999, loss = 0.00000696\n",
            "epoch 49: w = 1.999, loss = 0.00000503\n",
            "epoch 50: w = 1.999, loss = 0.00000363\n",
            "epoch 51: w = 1.999, loss = 0.00000262\n",
            "epoch 52: w = 2.000, loss = 0.00000190\n",
            "epoch 53: w = 2.000, loss = 0.00000137\n",
            "epoch 54: w = 2.000, loss = 0.00000099\n",
            "epoch 55: w = 2.000, loss = 0.00000071\n",
            "epoch 56: w = 2.000, loss = 0.00000052\n",
            "epoch 57: w = 2.000, loss = 0.00000037\n",
            "epoch 58: w = 2.000, loss = 0.00000027\n",
            "epoch 59: w = 2.000, loss = 0.00000019\n",
            "epoch 60: w = 2.000, loss = 0.00000014\n",
            "epoch 61: w = 2.000, loss = 0.00000010\n",
            "epoch 62: w = 2.000, loss = 0.00000007\n",
            "epoch 63: w = 2.000, loss = 0.00000005\n",
            "epoch 64: w = 2.000, loss = 0.00000004\n",
            "epoch 65: w = 2.000, loss = 0.00000003\n",
            "epoch 66: w = 2.000, loss = 0.00000002\n",
            "epoch 67: w = 2.000, loss = 0.00000001\n",
            "epoch 68: w = 2.000, loss = 0.00000001\n",
            "epoch 69: w = 2.000, loss = 0.00000001\n",
            "epoch 70: w = 2.000, loss = 0.00000001\n",
            "epoch 71: w = 2.000, loss = 0.00000000\n",
            "epoch 72: w = 2.000, loss = 0.00000000\n",
            "epoch 73: w = 2.000, loss = 0.00000000\n",
            "epoch 74: w = 2.000, loss = 0.00000000\n",
            "epoch 75: w = 2.000, loss = 0.00000000\n",
            "epoch 76: w = 2.000, loss = 0.00000000\n",
            "epoch 77: w = 2.000, loss = 0.00000000\n",
            "epoch 78: w = 2.000, loss = 0.00000000\n",
            "epoch 79: w = 2.000, loss = 0.00000000\n",
            "epoch 80: w = 2.000, loss = 0.00000000\n",
            "epoch 81: w = 2.000, loss = 0.00000000\n",
            "epoch 82: w = 2.000, loss = 0.00000000\n",
            "epoch 83: w = 2.000, loss = 0.00000000\n",
            "epoch 84: w = 2.000, loss = 0.00000000\n",
            "epoch 85: w = 2.000, loss = 0.00000000\n",
            "epoch 86: w = 2.000, loss = 0.00000000\n",
            "epoch 87: w = 2.000, loss = 0.00000000\n",
            "epoch 88: w = 2.000, loss = 0.00000000\n",
            "epoch 89: w = 2.000, loss = 0.00000000\n",
            "epoch 90: w = 2.000, loss = 0.00000000\n",
            "epoch 91: w = 2.000, loss = 0.00000000\n",
            "epoch 92: w = 2.000, loss = 0.00000000\n",
            "epoch 93: w = 2.000, loss = 0.00000000\n",
            "epoch 94: w = 2.000, loss = 0.00000000\n",
            "epoch 95: w = 2.000, loss = 0.00000000\n",
            "epoch 96: w = 2.000, loss = 0.00000000\n",
            "epoch 97: w = 2.000, loss = 0.00000000\n",
            "epoch 98: w = 2.000, loss = 0.00000000\n",
            "epoch 99: w = 2.000, loss = 0.00000000\n",
            "epoch 100: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "taY0NGBFpALY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace manually created loss and parameter updates with Pytorch Loss and Pytorch Optimizer \n",
        "\n",
        "general training pipeline, 3 steps: \n",
        "\n",
        "1. design model (inputsize, output size, forward pass \n",
        "2. construct loss and optimizer \n",
        "3. training loop\n",
        "  - forward pass: compute prediction\n",
        "  - Backward pass: gradients\n",
        "  - update weights "
      ],
      "metadata": {
        "id": "2pWcX0qRscPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "#f = w*x\n",
        "#f = 2*x\n",
        "X = torch.tensor([[1],[2],[3],[4]], dtype = torch.float32)\n",
        "Y = torch.tensor([[2],[4],[6],[8]], dtype = torch.float32)\n",
        "\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "#model = nn.Linear(input_size, output_size)\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(LinearRegression, self).__init__()\n",
        "    #define layers\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "#Calculate gradient \n",
        "\n",
        "\n",
        "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
        "\n",
        "#Training \n",
        "learning_rate = .015\n",
        "n_iters = 200\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #prediction = ford pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  #loss \n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  #Gradients = backward pass\n",
        "  l.backward() #dl/dw\n",
        "\n",
        "  #UPdate weights \n",
        "  #Don't want this opreationo be part of the computational graph ... wrap\n",
        "  optimizer.step()\n",
        "\n",
        "  #Must empty gradients, because whenever we write backward, we accumulate them in the w.grad_ attributes... so need to make sure gradients are 0 before next iteration \n",
        "  optimizer.zero_grad()\n",
        "\n",
        "\n",
        "  if epoch % 1 == 0: \n",
        "    [w,b] = model.parameters()\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVLEckn7tDzM",
        "outputId": "dfedc5c0-5255-404f-dfd9-7f748da31914"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "Prediction before training: f(5) = 0.068\n",
            "epoch 1: w = 0.584, loss = 31.81148529\n",
            "epoch 2: w = 0.931, loss = 17.87028503\n",
            "epoch 3: w = 1.191, loss = 10.03900051\n",
            "epoch 4: w = 1.386, loss = 5.63987541\n",
            "epoch 5: w = 1.532, loss = 3.16871977\n",
            "epoch 6: w = 1.642, loss = 1.78057671\n",
            "epoch 7: w = 1.724, loss = 1.00080013\n",
            "epoch 8: w = 1.786, loss = 0.56276619\n",
            "epoch 9: w = 1.832, loss = 0.31670156\n",
            "epoch 10: w = 1.867, loss = 0.17847285\n",
            "epoch 11: w = 1.893, loss = 0.10081942\n",
            "epoch 12: w = 1.912, loss = 0.05719383\n",
            "epoch 13: w = 1.927, loss = 0.03268259\n",
            "epoch 14: w = 1.938, loss = 0.01890882\n",
            "epoch 15: w = 1.946, loss = 0.01116659\n",
            "epoch 16: w = 1.952, loss = 0.00681267\n",
            "epoch 17: w = 1.957, loss = 0.00436207\n",
            "epoch 18: w = 1.961, loss = 0.00298069\n",
            "epoch 19: w = 1.963, loss = 0.00219995\n",
            "epoch 20: w = 1.965, loss = 0.00175668\n",
            "epoch 21: w = 1.967, loss = 0.00150300\n",
            "epoch 22: w = 1.968, loss = 0.00135587\n",
            "epoch 23: w = 1.969, loss = 0.00126865\n",
            "epoch 24: w = 1.970, loss = 0.00121510\n",
            "epoch 25: w = 1.971, loss = 0.00118052\n",
            "epoch 26: w = 1.971, loss = 0.00115664\n",
            "epoch 27: w = 1.971, loss = 0.00113879\n",
            "epoch 28: w = 1.972, loss = 0.00112439\n",
            "epoch 29: w = 1.972, loss = 0.00111196\n",
            "epoch 30: w = 1.972, loss = 0.00110066\n",
            "epoch 31: w = 1.972, loss = 0.00109005\n",
            "epoch 32: w = 1.973, loss = 0.00107986\n",
            "epoch 33: w = 1.973, loss = 0.00106996\n",
            "epoch 34: w = 1.973, loss = 0.00106023\n",
            "epoch 35: w = 1.973, loss = 0.00105066\n",
            "epoch 36: w = 1.973, loss = 0.00104121\n",
            "epoch 37: w = 1.973, loss = 0.00103185\n",
            "epoch 38: w = 1.973, loss = 0.00102259\n",
            "epoch 39: w = 1.974, loss = 0.00101342\n",
            "epoch 40: w = 1.974, loss = 0.00100433\n",
            "epoch 41: w = 1.974, loss = 0.00099533\n",
            "epoch 42: w = 1.974, loss = 0.00098641\n",
            "epoch 43: w = 1.974, loss = 0.00097756\n",
            "epoch 44: w = 1.974, loss = 0.00096881\n",
            "epoch 45: w = 1.974, loss = 0.00096012\n",
            "epoch 46: w = 1.974, loss = 0.00095151\n",
            "epoch 47: w = 1.975, loss = 0.00094299\n",
            "epoch 48: w = 1.975, loss = 0.00093454\n",
            "epoch 49: w = 1.975, loss = 0.00092616\n",
            "epoch 50: w = 1.975, loss = 0.00091786\n",
            "epoch 51: w = 1.975, loss = 0.00090964\n",
            "epoch 52: w = 1.975, loss = 0.00090149\n",
            "epoch 53: w = 1.975, loss = 0.00089341\n",
            "epoch 54: w = 1.975, loss = 0.00088541\n",
            "epoch 55: w = 1.975, loss = 0.00087747\n",
            "epoch 56: w = 1.976, loss = 0.00086961\n",
            "epoch 57: w = 1.976, loss = 0.00086181\n",
            "epoch 58: w = 1.976, loss = 0.00085409\n",
            "epoch 59: w = 1.976, loss = 0.00084644\n",
            "epoch 60: w = 1.976, loss = 0.00083885\n",
            "epoch 61: w = 1.976, loss = 0.00083133\n",
            "epoch 62: w = 1.976, loss = 0.00082388\n",
            "epoch 63: w = 1.976, loss = 0.00081650\n",
            "epoch 64: w = 1.976, loss = 0.00080918\n",
            "epoch 65: w = 1.977, loss = 0.00080193\n",
            "epoch 66: w = 1.977, loss = 0.00079475\n",
            "epoch 67: w = 1.977, loss = 0.00078762\n",
            "epoch 68: w = 1.977, loss = 0.00078056\n",
            "epoch 69: w = 1.977, loss = 0.00077357\n",
            "epoch 70: w = 1.977, loss = 0.00076663\n",
            "epoch 71: w = 1.977, loss = 0.00075976\n",
            "epoch 72: w = 1.977, loss = 0.00075296\n",
            "epoch 73: w = 1.977, loss = 0.00074621\n",
            "epoch 74: w = 1.977, loss = 0.00073952\n",
            "epoch 75: w = 1.978, loss = 0.00073290\n",
            "epoch 76: w = 1.978, loss = 0.00072632\n",
            "epoch 77: w = 1.978, loss = 0.00071982\n",
            "epoch 78: w = 1.978, loss = 0.00071337\n",
            "epoch 79: w = 1.978, loss = 0.00070697\n",
            "epoch 80: w = 1.978, loss = 0.00070064\n",
            "epoch 81: w = 1.978, loss = 0.00069436\n",
            "epoch 82: w = 1.978, loss = 0.00068814\n",
            "epoch 83: w = 1.978, loss = 0.00068197\n",
            "epoch 84: w = 1.978, loss = 0.00067586\n",
            "epoch 85: w = 1.979, loss = 0.00066980\n",
            "epoch 86: w = 1.979, loss = 0.00066380\n",
            "epoch 87: w = 1.979, loss = 0.00065785\n",
            "epoch 88: w = 1.979, loss = 0.00065196\n",
            "epoch 89: w = 1.979, loss = 0.00064612\n",
            "epoch 90: w = 1.979, loss = 0.00064033\n",
            "epoch 91: w = 1.979, loss = 0.00063459\n",
            "epoch 92: w = 1.979, loss = 0.00062890\n",
            "epoch 93: w = 1.979, loss = 0.00062326\n",
            "epoch 94: w = 1.979, loss = 0.00061768\n",
            "epoch 95: w = 1.980, loss = 0.00061214\n",
            "epoch 96: w = 1.980, loss = 0.00060666\n",
            "epoch 97: w = 1.980, loss = 0.00060122\n",
            "epoch 98: w = 1.980, loss = 0.00059583\n",
            "epoch 99: w = 1.980, loss = 0.00059049\n",
            "epoch 100: w = 1.980, loss = 0.00058520\n",
            "epoch 101: w = 1.980, loss = 0.00057996\n",
            "epoch 102: w = 1.980, loss = 0.00057476\n",
            "epoch 103: w = 1.980, loss = 0.00056961\n",
            "epoch 104: w = 1.980, loss = 0.00056450\n",
            "epoch 105: w = 1.980, loss = 0.00055945\n",
            "epoch 106: w = 1.980, loss = 0.00055443\n",
            "epoch 107: w = 1.981, loss = 0.00054947\n",
            "epoch 108: w = 1.981, loss = 0.00054454\n",
            "epoch 109: w = 1.981, loss = 0.00053966\n",
            "epoch 110: w = 1.981, loss = 0.00053482\n",
            "epoch 111: w = 1.981, loss = 0.00053003\n",
            "epoch 112: w = 1.981, loss = 0.00052528\n",
            "epoch 113: w = 1.981, loss = 0.00052057\n",
            "epoch 114: w = 1.981, loss = 0.00051591\n",
            "epoch 115: w = 1.981, loss = 0.00051128\n",
            "epoch 116: w = 1.981, loss = 0.00050670\n",
            "epoch 117: w = 1.981, loss = 0.00050216\n",
            "epoch 118: w = 1.982, loss = 0.00049766\n",
            "epoch 119: w = 1.982, loss = 0.00049321\n",
            "epoch 120: w = 1.982, loss = 0.00048878\n",
            "epoch 121: w = 1.982, loss = 0.00048440\n",
            "epoch 122: w = 1.982, loss = 0.00048006\n",
            "epoch 123: w = 1.982, loss = 0.00047576\n",
            "epoch 124: w = 1.982, loss = 0.00047150\n",
            "epoch 125: w = 1.982, loss = 0.00046727\n",
            "epoch 126: w = 1.982, loss = 0.00046308\n",
            "epoch 127: w = 1.982, loss = 0.00045893\n",
            "epoch 128: w = 1.982, loss = 0.00045482\n",
            "epoch 129: w = 1.982, loss = 0.00045075\n",
            "epoch 130: w = 1.982, loss = 0.00044671\n",
            "epoch 131: w = 1.983, loss = 0.00044270\n",
            "epoch 132: w = 1.983, loss = 0.00043873\n",
            "epoch 133: w = 1.983, loss = 0.00043480\n",
            "epoch 134: w = 1.983, loss = 0.00043090\n",
            "epoch 135: w = 1.983, loss = 0.00042704\n",
            "epoch 136: w = 1.983, loss = 0.00042322\n",
            "epoch 137: w = 1.983, loss = 0.00041942\n",
            "epoch 138: w = 1.983, loss = 0.00041566\n",
            "epoch 139: w = 1.983, loss = 0.00041194\n",
            "epoch 140: w = 1.983, loss = 0.00040825\n",
            "epoch 141: w = 1.983, loss = 0.00040459\n",
            "epoch 142: w = 1.983, loss = 0.00040097\n",
            "epoch 143: w = 1.983, loss = 0.00039737\n",
            "epoch 144: w = 1.984, loss = 0.00039381\n",
            "epoch 145: w = 1.984, loss = 0.00039028\n",
            "epoch 146: w = 1.984, loss = 0.00038678\n",
            "epoch 147: w = 1.984, loss = 0.00038332\n",
            "epoch 148: w = 1.984, loss = 0.00037988\n",
            "epoch 149: w = 1.984, loss = 0.00037648\n",
            "epoch 150: w = 1.984, loss = 0.00037311\n",
            "epoch 151: w = 1.984, loss = 0.00036976\n",
            "epoch 152: w = 1.984, loss = 0.00036645\n",
            "epoch 153: w = 1.984, loss = 0.00036316\n",
            "epoch 154: w = 1.984, loss = 0.00035991\n",
            "epoch 155: w = 1.984, loss = 0.00035668\n",
            "epoch 156: w = 1.984, loss = 0.00035349\n",
            "epoch 157: w = 1.984, loss = 0.00035032\n",
            "epoch 158: w = 1.985, loss = 0.00034718\n",
            "epoch 159: w = 1.985, loss = 0.00034407\n",
            "epoch 160: w = 1.985, loss = 0.00034098\n",
            "epoch 161: w = 1.985, loss = 0.00033793\n",
            "epoch 162: w = 1.985, loss = 0.00033490\n",
            "epoch 163: w = 1.985, loss = 0.00033190\n",
            "epoch 164: w = 1.985, loss = 0.00032892\n",
            "epoch 165: w = 1.985, loss = 0.00032598\n",
            "epoch 166: w = 1.985, loss = 0.00032306\n",
            "epoch 167: w = 1.985, loss = 0.00032016\n",
            "epoch 168: w = 1.985, loss = 0.00031729\n",
            "epoch 169: w = 1.985, loss = 0.00031445\n",
            "epoch 170: w = 1.985, loss = 0.00031163\n",
            "epoch 171: w = 1.985, loss = 0.00030884\n",
            "epoch 172: w = 1.986, loss = 0.00030607\n",
            "epoch 173: w = 1.986, loss = 0.00030333\n",
            "epoch 174: w = 1.986, loss = 0.00030061\n",
            "epoch 175: w = 1.986, loss = 0.00029792\n",
            "epoch 176: w = 1.986, loss = 0.00029525\n",
            "epoch 177: w = 1.986, loss = 0.00029260\n",
            "epoch 178: w = 1.986, loss = 0.00028998\n",
            "epoch 179: w = 1.986, loss = 0.00028738\n",
            "epoch 180: w = 1.986, loss = 0.00028481\n",
            "epoch 181: w = 1.986, loss = 0.00028225\n",
            "epoch 182: w = 1.986, loss = 0.00027972\n",
            "epoch 183: w = 1.986, loss = 0.00027721\n",
            "epoch 184: w = 1.986, loss = 0.00027473\n",
            "epoch 185: w = 1.986, loss = 0.00027227\n",
            "epoch 186: w = 1.986, loss = 0.00026983\n",
            "epoch 187: w = 1.986, loss = 0.00026741\n",
            "epoch 188: w = 1.987, loss = 0.00026502\n",
            "epoch 189: w = 1.987, loss = 0.00026264\n",
            "epoch 190: w = 1.987, loss = 0.00026029\n",
            "epoch 191: w = 1.987, loss = 0.00025795\n",
            "epoch 192: w = 1.987, loss = 0.00025564\n",
            "epoch 193: w = 1.987, loss = 0.00025335\n",
            "epoch 194: w = 1.987, loss = 0.00025108\n",
            "epoch 195: w = 1.987, loss = 0.00024883\n",
            "epoch 196: w = 1.987, loss = 0.00024660\n",
            "epoch 197: w = 1.987, loss = 0.00024439\n",
            "epoch 198: w = 1.987, loss = 0.00024220\n",
            "epoch 199: w = 1.987, loss = 0.00024003\n",
            "epoch 200: w = 1.987, loss = 0.00023788\n",
            "Prediction after training: f(5) = 9.974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement Linear Regression"
      ],
      "metadata": {
        "id": "zpw0lgIzytO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np \n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ybTBY3W9t20C"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0. Prepare Data\n",
        "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features =1, noise = 20, random_state =1)\n",
        "\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "y = y.view(y.shape[0], 1)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "#1. model\n",
        "\n",
        "#use built in linear model\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "\n",
        "\n",
        "#2. Loss and optimizer\n",
        "\n",
        "learning_rate = .01\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "#3. Training Loop\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  #forward pass and loss\n",
        "  y_predicted = model(X)\n",
        "  loss = criterion(y_predicted, y)\n",
        "\n",
        "  #Backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  #Update\n",
        "  optimizer.step()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "\n",
        "  if (epoch+1)%10 ==0:\n",
        "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "#plot\n",
        "predicted = model(X).detach().numpy()\n",
        "plt.plot(X_numpy, y_numpy, 'ro')\n",
        "plt.plot(X_numpy, predicted, 'b')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "k_1wpKh2y7Pn",
        "outputId": "eb6a0eb4-94d7-43f3-fb4b-032c92a043cd"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss = 4400.1929\n",
            "epoch: 20, loss = 3280.9666\n",
            "epoch: 30, loss = 2471.6985\n",
            "epoch: 40, loss = 1885.8953\n",
            "epoch: 50, loss = 1461.4125\n",
            "epoch: 60, loss = 1153.5300\n",
            "epoch: 70, loss = 930.0215\n",
            "epoch: 80, loss = 767.6320\n",
            "epoch: 90, loss = 649.5594\n",
            "epoch: 100, loss = 563.6508\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "coR9eGGt05gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression \n",
        "\n"
      ],
      "metadata": {
        "id": "GSmrQ6uI1MiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import numpy\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "#)0 Prepare data\n",
        "\n",
        "bc = datasets.load_breast_cancer()\n",
        "X,y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .2, random_state = 1234)\n",
        "\n",
        "#scale features\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "#convert to torch tensors\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "#reshape tensor from 1 row to 1 column\n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "y_test = y_test.view(y_test.shape[0], 1)\n",
        "\n",
        "\n",
        "\n",
        "#1) Model \n",
        "\n",
        "#f = wx+b, sigmoid at the end\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "  def __init__(self, n_input_features):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    self.linear = nn.Linear(n_input_features, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    y_predicted = torch.sigmoid(self.linear(x))\n",
        "    return y_predicted\n",
        "\n",
        "model = LogisticRegression(n_features)\n",
        "                           \n",
        "#2) Optimizer and loss\n",
        "\n",
        "learning_rate = .01\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "#3) \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  #forward pass and loss\n",
        "  y_predicted = model(X_train)\n",
        "  loss = criterion(y_predicted, y_train)\n",
        "\n",
        "  #backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  #updates\n",
        "  optimizer.step()\n",
        "\n",
        "  #zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  y_predicted = model(X_test)\n",
        "  y_predicted_cls = y_predicted.round()\n",
        "  acc = y_predicted_cls.eq(y_test).sum()/float(y_test.shape[0])\n",
        "\n",
        "  print(f'accuracy = {acc:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKmIRCzn1OAl",
        "outputId": "dd81d8c4-5c63-4633-863a-f91354cf0e44"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss = 0.2935\n",
            "epoch: 20, loss = 0.1805\n",
            "epoch: 30, loss = 0.1364\n",
            "epoch: 40, loss = 0.1144\n",
            "epoch: 50, loss = 0.1013\n",
            "epoch: 60, loss = 0.0925\n",
            "epoch: 70, loss = 0.0859\n",
            "epoch: 80, loss = 0.0807\n",
            "epoch: 90, loss = 0.0763\n",
            "epoch: 100, loss = 0.0726\n",
            "accuracy = 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ggzur3IkaIyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#play around with \n",
        " #- learning rate, num_epochs, diff optimizer \n"
      ],
      "metadata": {
        "id": "4E28daff4vwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Yn-Jb1-CaUC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vANhnKFQa6sf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}